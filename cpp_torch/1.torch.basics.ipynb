{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cling::DynamicLibraryManager::loadLibrary(): dlopen(/Users/zqfang/Github/XCpp/libtorch/lib/libtorch_cpu.dylib, 9): Library not loaded: @rpath/libiomp5.dylib\n",
      "  Referenced from: /Users/zqfang/Github/XCpp/libtorch/lib/libtorch_cpu.dylib\n",
      "  Reason: image not found\n",
      "cling::DynamicLibraryManager::loadLibrary(): dlopen(/Users/zqfang/Github/XCpp/libtorch/lib/libtorch.dylib, 9): Library not loaded: @rpath/libtorch_cpu.dylib\n",
      "  Referenced from: /Users/zqfang/Github/XCpp/libtorch/lib/libtorch.dylib\n",
      "  Reason: image not found\n",
      "cling::DynamicLibraryManager::loadLibrary(): dlopen(/Users/zqfang/Github/XCpp/libtorch/lib/libtorch_global_deps.dylib, 9): Library not loaded: @rpath/libiomp5.dylib\n",
      "  Referenced from: /Users/zqfang/Github/XCpp/libtorch/lib/libtorch_global_deps.dylib\n",
      "  Reason: image not found\n",
      "cling::DynamicLibraryManager::loadLibrary(): dlopen(/Users/zqfang/Github/XCpp/libtorch/lib/libtorch_python.dylib, 9): Library not loaded: @rpath/libshm.dylib\n",
      "  Referenced from: /Users/zqfang/Github/XCpp/libtorch/lib/libtorch_python.dylib\n",
      "  Reason: image not found\n",
      "cling::DynamicLibraryManager::loadLibrary(): dlopen(/Users/zqfang/Github/XCpp/libtorch/lib/libshm.dylib, 9): Library not loaded: @rpath/libiomp5.dylib\n",
      "  Referenced from: /Users/zqfang/Github/XCpp/libtorch/lib/libshm.dylib\n",
      "  Reason: image not found\n",
      "cling::DynamicLibraryManager::loadLibrary(): dlopen(/Users/zqfang/Github/XCpp/libtorch/lib/libcaffe2_detectron_ops.dylib, 9): Library not loaded: @rpath/libiomp5.dylib\n",
      "  Referenced from: /Users/zqfang/Github/XCpp/libtorch/lib/libcaffe2_detectron_ops.dylib\n",
      "  Reason: image not found\n",
      "cling::DynamicLibraryManager::loadLibrary(): dlopen(/Users/zqfang/Github/XCpp/libtorch/lib/libcaffe2_module_test_dynamic.dylib, 9): Library not loaded: @rpath/libiomp5.dylib\n",
      "  Referenced from: /Users/zqfang/Github/XCpp/libtorch/lib/libcaffe2_module_test_dynamic.dylib\n",
      "  Reason: image not found\n",
      "cling::DynamicLibraryManager::loadLibrary(): dlopen(/Users/zqfang/Github/XCpp/libtorch/lib/libcaffe2_observers.dylib, 9): Library not loaded: @rpath/libiomp5.dylib\n",
      "  Referenced from: /Users/zqfang/Github/XCpp/libtorch/lib/libcaffe2_observers.dylib\n",
      "  Reason: image not found\n"
     ]
    }
   ],
   "source": [
    "// libTorch 1.5\n",
    "// Load all LibTorch dependencies for cling\n",
    "#pragma cling add_library_path(\"/Users/zqfang/Github/XCpp/libtorch/lib\")\n",
    "#pragma cling add_include_path(\"/Users/zqfang/Github/XCpp/libtorch/include/\")\n",
    "#pragma cling add_include_path(\"/Users/zqfang/Github/XCpp/libtorch/include/torch/csrc/api/include/\")\n",
    "\n",
    "// Note: This is loaded for macOS environment, for Linux try loading *.so files\n",
    "#pragma cling load(\"/Users/zqfang/Github/XCpp/libtorch/lib/libc10.dylib\")\n",
    "#pragma cling load(\"/Users/zqfang/Github/XCpp/libtorch/lib/libiomp5.dylib\")\n",
    "#pragma cling load(\"/Users/zqfang/Github/XCpp/libtorch/lib/libtorch_cpu.dylib\")\n",
    "#pragma cling load(\"/Users/zqfang/Github/XCpp/libtorch/lib/libtorch.dylib\")\n",
    "#pragma cling load(\"/Users/zqfang/Github/XCpp/libtorch/lib/libtorch_global_deps.dylib\")\n",
    "\n",
    "#pragma cling load(\"/Users/zqfang/Github/XCpp/libtorch/lib/libfbjni.dylib\")\n",
    "#pragma cling load(\"/Users/zqfang/Github/XCpp/libtorch/lib/libpytorch_jni.dylib\")\n",
    "#pragma cling load(\"/Users/zqfang/Github/XCpp/libtorch/lib/libtorch_python.dylib\")\n",
    "#pragma cling load(\"/Users/zqfang/Github/XCpp/libtorch/lib/libshm.dylib\")\n",
    "\n",
    "#pragma cling load(\"/Users/zqfang/Github/XCpp/libtorch/lib/libcaffe2_detectron_ops.dylib\")\n",
    "#pragma cling load(\"/Users/zqfang/Github/XCpp/libtorch/lib/libcaffe2_module_test_dynamic.dylib\")\n",
    "#pragma cling load(\"/Users/zqfang/Github/XCpp/libtorch/lib/libcaffe2_observers.dylib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <torch/torch.h>\n",
    "#include <torch/script.h>\n",
    "#include <iostream>\n",
    "#include <iomanip>\n",
    "\n",
    "void print_tensor_size(const torch::Tensor&);\n",
    "void print_script_module(const torch::jit::script::Module& module, size_t spaces = 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Basics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std::cout << \"PyTorch Basics\\n\\n\";\n",
    "\n",
    "// Set floating point output precision\n",
    "std::cout << std::fixed << std::setprecision(4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- BASIC AUTOGRAD EXAMPLE 1 ----\n",
      "2\n",
      "[ CPUFloatType{} ]\n",
      "1\n",
      "[ CPUFloatType{} ]\n",
      "1\n",
      "[ CPUFloatType{} ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "@0x107267ed0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std::cout << \"---- BASIC AUTOGRAD EXAMPLE 1 ----\\n\";\n",
    "\n",
    "// Create Tensors\n",
    "torch::Tensor x = torch::tensor(1.0, torch::requires_grad());\n",
    "torch::Tensor w = torch::tensor(2.0, torch::requires_grad());\n",
    "torch::Tensor b = torch::tensor(3.0, torch::requires_grad());\n",
    "\n",
    "// Build a computational graph\n",
    "auto y = w * x + b;  // y = 2 * x + 3\n",
    "\n",
    "// Compute the gradients\n",
    "y.backward();\n",
    "\n",
    "// Print out the gradients\n",
    "std::cout << x.grad() << '\\n';  // x.grad() = 2\n",
    "std::cout << w.grad() << '\\n';  // w.grad() = 1\n",
    "std::cout << b.grad() << \"\\n\\n\";  // b.grad() = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- BASIC AUTOGRAD EXAMPLE 2 ----\n",
      "w:\n",
      "-0.2216 -0.0401 -0.0063\n",
      " 0.5274 -0.2586  0.5540\n",
      "[ CPUFloatType{2,3} ]\n",
      "b:\n",
      "-0.2209\n",
      " 0.1046\n",
      "[ CPUFloatType{2} ]\n",
      "Loss: 2.0071\n",
      "dL/dw:\n",
      "-0.3324  0.4685  0.7034\n",
      " 0.7355 -0.7133  0.9054\n",
      "[ CPUFloatType{2,3} ]\n",
      "dL/db:\n",
      "-0.4277\n",
      "-0.3347\n",
      "[ CPUFloatType{2} ]\n",
      "Loss after 1 optimization step: 1.9774\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std::cout << \"---- BASIC AUTOGRAD EXAMPLE 2 ----\\n\";\n",
    "\n",
    "// Create Tensors of shapes\n",
    "x = torch::randn({10, 3});\n",
    "y = torch::randn({10, 2});\n",
    "\n",
    "// Build a fully connected layer\n",
    "torch::nn::Linear linear(3, 2);\n",
    "std::cout << \"w:\\n\" << linear->weight << '\\n';\n",
    "std::cout << \"b:\\n\" << linear->bias << '\\n';\n",
    "\n",
    "// Create loss function and optimizer\n",
    "torch::nn::MSELoss criterion;\n",
    "torch::optim::SGD optimizer(linear->parameters(), torch::optim::SGDOptions(0.01));\n",
    "\n",
    "// Forward pass\n",
    "auto pred = linear->forward(x);\n",
    "\n",
    "// Compute loss\n",
    "auto loss = criterion(pred, y);\n",
    "std::cout << \"Loss: \" << loss.item<double>() << '\\n';\n",
    "\n",
    "// Backward pass\n",
    "loss.backward();\n",
    "\n",
    "// Print out the gradients\n",
    "std::cout << \"dL/dw:\\n\" << linear->weight.grad() << '\\n';\n",
    "std::cout << \"dL/db:\\n\" << linear->bias.grad() << '\\n';\n",
    "\n",
    "// 1 step gradient descent\n",
    "optimizer.step();\n",
    "\n",
    "// Print out the loss after 1-step gradient descent\n",
    "pred = linear->forward(x);\n",
    "loss = criterion(pred, y);\n",
    "std::cout << \"Loss after 1 optimization step: \" << loss.item<double>() << \"\\n\\n\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- CREATING TENSORS FROM EXISTING DATA ----\n",
      "Tensor from array:\n",
      " 1  2\n",
      " 3  4\n",
      "[ CPUFloatType{2,2} ]\n",
      "Tensor from vector:\n",
      " 1  2\n",
      " 3  4\n",
      "[ CPUFloatType{2,2} ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std::cout << \"---- CREATING TENSORS FROM EXISTING DATA ----\\n\";\n",
    "\n",
    "// WARNING: Tensors created with torch::from_blob(ptr_to_data, ...) do not own\n",
    "// the memory pointed to by ptr_to_data!\n",
    "// (see https://pytorch.org/cppdocs/notes/tensor_basics.html#using-externally-created-data)\n",
    "//\n",
    "// If you want a tensor that has its own copy of the data you can call clone() on the\n",
    "// tensor returned from torch::from_blob(), e.g.:\n",
    "// torch::Tensor t = torch::from_blob(ptr_to_data, ...).clone();\n",
    "// (see https://github.com/pytorch/pytorch/issues/12506#issuecomment-429573396)\n",
    "\n",
    "// Tensor From C-style array\n",
    "float data_array[] = {1, 2, 3, 4};\n",
    "torch::Tensor t1 = torch::from_blob(data_array, {2, 2});\n",
    "std::cout << \"Tensor from array:\\n\" << t1 << '\\n';\n",
    "\n",
    "TORCH_CHECK(data_array == t1.data_ptr<float>());\n",
    "\n",
    "// Tensor from vector:\n",
    "std::vector<float> data_vector = {1, 2, 3, 4};\n",
    "torch::Tensor t2 = torch::from_blob(data_vector.data(), {2, 2});\n",
    "std::cout << \"Tensor from vector:\\n\" << t2 << \"\\n\\n\";\n",
    "\n",
    "TORCH_CHECK(data_vector.data() == t2.data_ptr<float>());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- SLICING AND EXTRACTING PARTS FROM TENSORS ----\n",
      "s:\n",
      " 1  2  3\n",
      " 4  5  6\n",
      " 7  8  9\n",
      "[ CPULongType{3,3} ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "@0x107267ed0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std::cout << \"---- SLICING AND EXTRACTING PARTS FROM TENSORS ----\\n\";\n",
    "\n",
    "std::vector<int64_t> test_data = {1, 2, 3, 4, 5, 6, 7, 8, 9};\n",
    "torch::Tensor s = torch::from_blob(test_data.data(), {3, 3}, torch::kInt64);\n",
    "std::cout << \"s:\\n\" << s << '\\n';\n",
    "// Output:\n",
    "// 1 2 3\n",
    "// 4 5 6\n",
    "// 7 8 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// using torch::indexing::Slice;\n",
    "// using torch::indexing::None;\n",
    "// using torch::indexing::Ellipsis;\n",
    "using namespace torch::indexing;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"s[0,2]\" as tensor:\n",
      "3\n",
      "[ CPULongType{} ]\n",
      "\"s[0,2]\" as value:\n",
      "3\n",
      "\"s[:,2]\":\n",
      " 3\n",
      " 6\n",
      " 9\n",
      "[ CPULongType{3} ]\n",
      "\"s[:2,:]\":\n",
      " 1  2  3\n",
      " 4  5  6\n",
      "[ CPULongType{2,3} ]\n",
      "\"s[:,1:]\":\n",
      " 2  3\n",
      " 5  6\n",
      " 8  9\n",
      "[ CPULongType{3,2} ]\n",
      "\"s[:,::2]\":\n",
      " 1  3\n",
      " 4  6\n",
      " 7  9\n",
      "[ CPULongType{3,2} ]\n",
      "\"s[:2,1]\":\n",
      " 2\n",
      " 5\n",
      "[ CPULongType{2} ]\n",
      "\"s[..., :2]\":\n",
      " 1  2\n",
      " 4  5\n",
      " 7  8\n",
      "[ CPULongType{3,2} ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "@0x107267ed0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Indexing and slicing tensors is done very similarly to how\n",
    "// it is done in Python.\n",
    "//\n",
    "// For a complete side-by-side translation of all indexing types, see:\n",
    "// https://pytorch.org/cppdocs/notes/tensor_indexing.html\n",
    "\n",
    "// Extract a single element tensor:\n",
    "std::cout << \"\\\"s[0,2]\\\" as tensor:\\n\" << s.index({0, 2}) << '\\n';\n",
    "std::cout << \"\\\"s[0,2]\\\" as value:\\n\" << s.index({0, 2}).item<int64_t>() << '\\n';\n",
    "// Output:\n",
    "// 3\n",
    "\n",
    "// Slice a tensor along a dimension at a given index.\n",
    "std::cout << \"\\\"s[:,2]\\\":\\n\" << s.index({Slice(), 2}) << '\\n';\n",
    "// Output:\n",
    "// 3\n",
    "// 6\n",
    "// 9\n",
    "\n",
    "// Slice a tensor along a dimension at given indices from\n",
    "// a start-index up to - but not including - an end-index using a given step size.\n",
    "std::cout << \"\\\"s[:2,:]\\\":\\n\" << s.index({Slice(None, 2), Slice()}) << '\\n';\n",
    "// Output:\n",
    "// 1 2 3\n",
    "// 4 5 6\n",
    "std::cout << \"\\\"s[:,1:]\\\":\\n\" << s.index({Slice(), Slice(1, None)}) << '\\n';\n",
    "// Output:\n",
    "// 2 3\n",
    "// 5 6\n",
    "// 8 9\n",
    "std::cout << \"\\\"s[:,::2]\\\":\\n\" << s.index({Slice(), Slice(None, None, 2)}) << '\\n';\n",
    "// Output:\n",
    "// 1 3\n",
    "// 4 6\n",
    "// 7 9\n",
    "\n",
    "// Combination.\n",
    "std::cout << \"\\\"s[:2,1]\\\":\\n\" << s.index({Slice(None, 2), 1}) << '\\n';\n",
    "// Output:\n",
    "// 2\n",
    "// 5\n",
    "\n",
    "// Ellipsis (...).\n",
    "std::cout << \"\\\"s[..., :2]\\\":\\n\" << s.index({Ellipsis, Slice(None, 2)}) << \"\\n\\n\";\n",
    "// Output:\n",
    "// 1 2\n",
    "// 4 5\n",
    "// 7 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// =============================================================== //\n",
    "//                         INPUT PIPELINE                          //\n",
    "// =============================================================== //\n",
    "\n",
    "std::cout << \"---- INPUT PIPELINE ----\\n\";\n",
    "\n",
    "// Construct MNIST dataset\n",
    "const std::string MNIST_data_path = \"data/mnist/\";\n",
    "\n",
    "auto dataset = torch::data::datasets::MNIST(MNIST_data_path)\n",
    "    .map(torch::data::transforms::Normalize<>(0.1307, 0.3081))\n",
    "    .map(torch::data::transforms::Stack<>());\n",
    "\n",
    "// Fetch one data pair\n",
    "auto example = dataset.get_batch(0);\n",
    "std::cout << \"Sample data size: \";\n",
    "std::cout << example.data.sizes() << \"\\n\";\n",
    "std::cout << \"Sample target: \" << example.target.item<int>() << \"\\n\";\n",
    "\n",
    "// Construct data loader\n",
    "auto dataloader = torch::data::make_data_loader<torch::data::samplers::RandomSampler>(\n",
    "    dataset, 64);\n",
    "\n",
    "// Fetch a mini-batch\n",
    "auto example_batch = *dataloader->begin();\n",
    "std::cout << \"Sample batch - data size: \";\n",
    "std::cout << example_batch.data.sizes() << \"\\n\";\n",
    "std::cout << \"Sample batch - target size: \";\n",
    "std::cout << example_batch.target.sizes() << \"\\n\\n\";\n",
    "// Actual usage of the dataloader:\n",
    "// for (auto& batch : *dataloader) {\n",
    "    // Training code here\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
